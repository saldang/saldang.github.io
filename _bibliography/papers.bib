Scopus
EXPORT DATE: 27 April 2024

@ARTICLE{Balzanella202031,
	author = {Balzanella, Antonio and D’Angelo, Salvatore and Iacono, Mauro and Nacchia, Stefania and Verde, Rosanna},
	title = {Automatic Classification of Road Traffic with Fiber Based Sensors in Smart Cities Applications},
	year = {2020},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12252 LNCS},
	pages = {31 – 46},
	doi = {10.1007/978-3-030-58811-3_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092227136&doi=10.1007%2f978-3-030-58811-3_3&partnerID=40&md5=9be5e3e3c2bf062e03c8203ad4efc252},
	abstract = {Low cost monitoring of road traffic can bring a significant contribution to use the smart cities perspective for safety. The possibility of sensing and classifying vehicles and march conditions by means of simple physical sensors may support both real time applications and studies on traffic dynamics, e.g. support and assistance for car crashes and prevention of accidents, and maintenance planning or support to trials in case of litigation. Optical fibers technology is well known for its wide adoption in data transmissions as a commodity component of computer networks: its popularity led to large availability on the market of high quality fiber at affordable price. As a purely physical application, its optical properties may be exploited to monitor in real time mechanical solicitations the fiber undergoes. In this paper we present a novel approach to using optical fibers as road sensors. As quite popular in literature, fiber is used to sense the vibrations caused by vehicles on the road: in our case, signals are processed by functional classification techniques to obtain a higher quality and a larger flexibility for the reuse of results. Classification aims at enabling profiling of road traffic. Moreover in our approach we would like to optimise the analysis and classification computations by splitting the process among edge nodes and cloud nodes according to the available computation capacity. Our solution has been tested by an experimental campaign to show the suitability of the approach. © 2020, Springer Nature Switzerland AG.},
	type = {Conference paper},
	source = {Scopus}
}

@CONFERENCE{DI Martino2018402,
	author = {DI Martino, Beniamino and D'Angelo, Salvatore and Esposito, Antonio and Cappuzzo, Riccardo and De Oliveira, Anderson Santana},
	title = {ROCK algorithm parallelization with TOREADOR primitives},
	year = {2018},
	journal = {Proceedings - 32nd IEEE International Conference on Advanced Information Networking and Applications Workshops, WAINA 2018},
	volume = {2018-January},
	pages = {402 – 407},
	doi = {10.1109/WAINA.2018.00119},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056263065&doi=10.1109%2fWAINA.2018.00119&partnerID=40&md5=f185d4f61e89787347624ae7e3bba6c8},
	abstract = {We present the benefits of applying the code once deploy everywhere approach to clustering of categorical data over large datasets. The paper brings two main contributions: an step-by step application of the code based approach and an enhancement for the ROCK algorithm for clustering categorical data. © 2018 IEEE.},
	type = {Conference paper},
	source = {Scopus}
}

@ARTICLE{Horn2023146,
	author = {Horn, Geir and Di Martino, Beniamino and D’Angelo, Salvatore and Esposito, Antonio},
	title = {Programming Paradigms for the Cloud Continuum},
	year = {2023},
	journal = {Lecture Notes in Networks and Systems},
	volume = {655 LNNS},
	pages = {146 – 156},
	doi = {10.1007/978-3-031-28694-0_14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151045597&doi=10.1007%2f978-3-031-28694-0_14&partnerID=40&md5=79d29837ea9667fd726506b71aea9358},
	abstract = {The Cloud continuum is a notion for the distributed infrastructure able to run third party software on heterogeneous hardware ranging from the high performance core Cloud data centres, through smaller fog data centres, down to resource constrained Edge servers. Beyond that, one will often have Internet of Things (IoT) devices on a wired or wireless infrastructure for data collection and actuation. The application components should be allocated along this continuum in order to optimize the application performance which is a combined measure of communication speed and computational power. However, most applications are not written with distribution in mind, and may not even be component based. This paper discusses various programming paradigms and how they can be extended to support distribution and embedding into the microservice architecture supporting tomorrow’s Cloud applications. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	type = {Conference paper},
	source = {Scopus}
}

@ARTICLE{Di Martino2020,
	author = {Di Martino, Beniamino and Venticinque, Salvatore and Esposito, Antonio and D'Angelo, Salvatore},
	title = {A methodology based on computational patterns for offloading of big data applications on cloud-edge platforms},
	year = {2020},
	journal = {Future Internet},
	volume = {12},
	number = {2},
	doi = {10.3390/fi12020028},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081125221&doi=10.3390%2ffi12020028&partnerID=40&md5=c12a6f8af99dcf10d2830157e895ca8a},
	abstract = {Internet of Things (IoT) is becoming a widespread reality, as interconnected smart devices and sensors have overtaken the IT market and invaded every aspect of the human life. This kind of development, while already foreseen by IT experts, implies additional stress to already congested networks, and may require further investments in computational power when considering centralized and Cloud based solutions. That is why a common trend is to rely on local resources, provided by smart devices themselves or by aggregators, to deal with part of the required computations: this is the base concept behind Fog Computing, which is becoming increasingly adopted as a distributed calculation solution. In this paper a methodology, initially developed within the TOREADOR European project for the distribution of Big Data computations over Cloud platforms, will be described and applied to an algorithm for the prediction of energy consumption on the basis of data coming from home sensors, already employed within the CoSSMic European Project. The objective is to demonstrate that, by applying such a methodology, it is possible to improve the calculation performances and reduce communication with centralized resources. © 2020 by the authors.},
	type = {Article},
	source = {Scopus},
	note = {All Open Access, Gold Open Access}
}

@ARTICLE{Branco2023127,
	author = {Branco, Dario and Di Martino, Beniamino and Cosconati, Sandro and Kranzlmueller, Dieter and D’Angelo, Salvatore},
	title = {Towards a Parallel Graph Approach to Drug Discovery},
	year = {2023},
	journal = {Lecture Notes in Networks and Systems},
	volume = {655 LNNS},
	pages = {127 – 135},
	doi = {10.1007/978-3-031-28694-0_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151057130&doi=10.1007%2f978-3-031-28694-0_12&partnerID=40&md5=06826c5a371caadb4773c3a0d996a3b7},
	abstract = {This work deals with the problem of recognising chemical entities that are able to act as polypharmacological compounds by binding and modulating two different proteins that have been demonstrated to be critical for cancer metastatic potential and aggressivity. In particular, the aim is to develop a method that automatically indicates which of the given compounds are likely to be ‘active’ with respect to the two proteins in question. In medicinal chemistry, an active compound is defined as a ligand that is capable of modulating the e-mail: dieter.kranzlmueller@lrz.de. In this work, on the other hand, we aim to develop a parallel algorithm that is able to provide an exact solution to the problem using classical techniques of isomorphism and similarity between graphs. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	type = {Conference paper},
	source = {Scopus}
}

@ARTICLE{Di Martino2022183,
	author = {Di Martino, Beniamino and D’Angelo, Salvatore and Esposito, Antonio and Lupi, Pietro},
	title = {Anomalous Witnesses and Registrations Detection in the Italian Justice System Based on Big Data and Machine Learning Techniques},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {451 LNNS},
	pages = {183 – 192},
	doi = {10.1007/978-3-030-99619-2_18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128754043&doi=10.1007%2f978-3-030-99619-2_18&partnerID=40&md5=7a1c969d4bb573954efa4d55dfd6bc5b},
	abstract = {It is estimated that in 2020 the amount of data produced was about 44 zettabytes for a per capita daily production of about 16 gigabytes. These numbers make us think about how much knowledge is possible to extract from the data produced every day by every single inhabitant of the earth. Supported by the growing diffusion of frameworks and tools for the automatic analysis of Big Data, Machine Learning and Deep Learning, we can try to extract knowledge from all this information and use it to offer a greater definition to human knowledge. In this paper we present two techniques that exploit the knowledge provided by data analysis to identify anomalies in the Italian judicial system, in particular in the civil process. The first anomaly concerns the presence of “serial witnesses”, people who lend themselves to provide testimony of the facts occurred in different trial proceedings where places dates and events overlap highlighting a false testimony. The second anomaly relates to “multiple entries” by lawyers with the aim of being able to happen upon a judge “favorable” to the case. The two anomalies presented, but the possibilities are endless, are identified through the definition of Big Data pipelines for data aggregation, information extraction and data analysis. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	type = {Conference paper},
	source = {Scopus}
}

@ARTICLE{Branco2024629,
	author = {Branco, Dario and Coppa, Antonio and D’Angelo, Salvatore and Gigli, Stefania Quilici and Renda, Giuseppina and Venticinque, Salvatore},
	title = {Advanced IT Technologies Applied to Archaeological Park of Norba (Latium, Italy)},
	year = {2024},
	journal = {Lecture Notes on Data Engineering and Communications Technologies},
	volume = {193},
	pages = {629 – 638},
	doi = {10.1007/978-3-031-53555-0_60},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186441074&doi=10.1007%2f978-3-031-53555-0_60&partnerID=40&md5=03fc6b7d89a0f87d84fce507269d1d63},
	abstract = {This study proposes the integrated utilization of advanced information technologies to enrich conservation, research, and visitors’ experience at the Norba Archaeological Park, located in Latium, Italy. Using 3D digitalization, augmented reality (AR), and geographical information systems (GIS) technologies, this work aims to transform the archaeological and historical heritage into material that can be used via smart devices, thus improving and expanding the methodologies of fruition. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.},
	type = {Book chapter},
	source = {Scopus}
}

@CONFERENCE{Di Martino2018408,
	author = {Di Martino, Beniamino and D'Angelo, Salvatore and Esposito, Antonio and Martinez, Ivàn and Montero, Jorge and Pariente, Tomas},
	title = {Parallelization and deployment of big data algorithms: The TOREADOR approach},
	year = {2018},
	journal = {Proceedings - 32nd IEEE International Conference on Advanced Information Networking and Applications Workshops, WAINA 2018},
	volume = {2018-January},
	pages = {408 – 412},
	doi = {10.1109/WAINA.2018.00120},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056288844&doi=10.1109%2fWAINA.2018.00120&partnerID=40&md5=4d7f113bc7ab81e1a5c4b4d6500af862},
	abstract = {In order to reduce the initial investments needed by small and medium enterprises (SMEs) to acquire the necessary expertise, hardware and software to run proper Big Data Analytics, TOREADOR proposes a Big Data Analytics framework which supports users in devising their own Big Data solutions by keeping the inherent costs at a minimum. Among the objectives of the TOREADOR framework is supporting developers in parallelizing and deploying their algorithms, in order to develop they own analytics solutions. This paper describes the Code-Based approach, developed by CINI and adopted within the TOREADOR framework to parallelize users' algorithms and deploy them on distributed platforms, with a focus on its integration with the web services and resources offered by ATOS for the actual deployment of the solution. © 2018 IEEE.},
	type = {Conference paper},
	source = {Scopus}
}

@ARTICLE{Di Martino2022435,
	author = {Di Martino, Beniamino and Colucci Cante, Luigi and D’Angelo, Salvatore and Esposito, Antonio and Graziano, Mariangela and Ammendolia, Rosario and Lupi, Pietro},
	title = {Semantic Based Knowledge Management in e-Government Document Workflows: A Case Study for Judiciary Domain in Road Accident Trials},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {497 LNNS},
	pages = {435 – 445},
	doi = {10.1007/978-3-031-08812-4_42},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133006480&doi=10.1007%2f978-3-031-08812-4_42&partnerID=40&md5=aa3cdf9fd2f0417e14f76f72175e480e},
	abstract = {Many of the activities carried out by Judges and Chancellors in Civil Trials consist in organizing the documentation attached to each Trial which they follow in order also to decide if the conditions for proposability of the application proposed in court exist, where the law lays down the requisites in this regard. With the advent of the Telematic Civil Process, all documentation is received digitally and therefore could be automatically organized and analyzed in order to facilitate the work of Judges and Chancellors. However, there is still no automatic method of organizing documents in the Italian Ministry of Justice systems and, although supported by a document organization software, Judges and registrars still have to search by reading the necessary information within the documents cataloged in such a way, not always presented uniformly by lawyers. In this paper a feasibility study for the creation of a prototype expert system has been carried out, with the objective to analyse the documentation relative to a specific category of Civil Trials, that is Road Accidents, to organize it according to a predefined document categorization, and to automatically determine the proposability of the instances presented to Courts. Natural Language Processing algorithms have been applied in order to examine the documentation, and logical rules have been designed, according to the current Italian legislation, to determine the admissibility of instances. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	type = {Conference paper},
	source = {Scopus}
}

@CONFERENCE{Ardagna2018174,
	author = {Ardagna, Claudio A. and Bellandi, Valerio and Ceravolo, Paolo and Damiani, Ernesto and Di Martino, Beniamino and D'Angelo, Salvatore and Esposito, Antonio},
	title = {A Fast and Incremental Development Life Cycle for Data Analytics as a Service},
	year = {2018},
	journal = {Proceedings - 2018 IEEE International Congress on Big Data, BigData Congress 2018 - Part of the 2018 IEEE World Congress on Services},
	pages = {174 – 181},
	doi = {10.1109/BigDataCongress.2018.00030},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057744680&doi=10.1109%2fBigDataCongress.2018.00030&partnerID=40&md5=0adeb53ccd3c1b524c98d7172ce13d1f},
	abstract = {Big Data does not only refer to a huge amount of diverse and heterogeneous data. It also points to the management of procedures, technologies, and competencies associated with the analysis of such data, with the aim of supporting high-quality decision making. There are, however, several obstacles to the effective management of a Big Data computation, such as data velocity, variety, and veracity, and technological complexity, which represent the main barriers towards the full adoption of the Big Data paradigm. The goal of this work is to define a new software Development Life Cycle for the design and implementation of a Big Data computation. Our proposal integrates two model-driven methods: a first method based on pre-configured services that reduces the cost of deployment and a second method based on custom component development that provides an incremental process of refinement and customization. The proposal is experimentally evaluated by clustering a data set of the distribution of the population in the United States based on contextual criteria. © 2018 IEEE.},
	type = {Conference paper},
	source = {Scopus},
	note = {All Open Access, Green Open Access}
}

@ARTICLE{Di Martino2022121,
	author = {Di Martino, Beniamino and Bombace, Vincenzo and D’Angelo, Salvatore and Esposito, Antonio},
	title = {A Microservices Based Architecture for the Sentiment Analysis of Tweets},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {451 LNNS},
	pages = {121 – 130},
	doi = {10.1007/978-3-030-99619-2_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128735551&doi=10.1007%2f978-3-030-99619-2_12&partnerID=40&md5=9f217a6139f6faf5d159882ae4b03f97},
	abstract = {Sentiment Analysis techniques have been largely applied to Tweets, newsgroups and Social Networks in general, with several applications in sociological studies. Users tend to comment and express their opinions much more genuinely on Social Networks, as if their natural filters were somehow lifted. In particular, complaints regarding malfunctions of specific services are often filed in form of public comments or Tweets, on the official accounts of the Service providers. In some cases, people just express dissatisfaction regarding services on their own accounts, and use hashtags to better identify the specific topic they are referring to. In this paper, a framework for the analysis of Tweets is proposed, with the specific objective to identify malfunctioning of essential services, such as water, electrical, gas or public illumination. Since the number of comments and Tweets to analyse is considerable, a microservices based architecture, with Docker containers and Kafka queues, has been created. This allows to define a scalable and parallelizable architecture, whose characteristics can be adapted to the number of Tweets to be analysed, which are in turn treated as a continuous data streaming. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	type = {Conference paper},
	source = {Scopus}
}

@CONFERENCE{Bandirali2018396,
	author = {Bandirali, Cesare and Lodi, Stefano and Moro, Gianluca and Pagliarani, Andrea and Sartori, Claudio and D'Angelo, Salvatore and Di Martino, Beniamino and Esposito, Antonio},
	title = {Parallel primitives for vendor-agnostic implementation of big data mining algorithms},
	year = {2018},
	journal = {Proceedings - 32nd IEEE International Conference on Advanced Information Networking and Applications Workshops, WAINA 2018},
	volume = {2018-January},
	pages = {396 – 401},
	doi = {10.1109/WAINA.2018.00118},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056281792&doi=10.1109%2fWAINA.2018.00118&partnerID=40&md5=8f09c28c624dffe0115641a3dc247d51},
	abstract = {In the age of Big Data, scalable algorithm implementations as well as powerful computational resources are required. For data mining and data analytics the support of big data platforms is becoming increasingly important, since they provide algorithm implementations with all the resources needed for their execution. However, choosing the best platform might depend on several constraints, including but not limited to computational resources, storage resources, target tasks, service costs. Sometimes it may be necessary to switch from one platform to another depending on the constraints. As a consequence, it is desirable to reuse as much algorithm code as possible, so as to simplify the setup in new target platforms. Unfortunately each big data platform has its own peculiarity, especially to deal with parallelism. This impacts on algorithm implementation, which generally needs to be modified before being executed. This work introduces functional parallel primitives to define the parallelizable parts of algorithms in a uniform way, independent of the target platform. Primitives are then transformed by a compiler into skeletons, which are finally deployed on vendor-dependent frameworks. The procedure proposed aids not only in terms of code reuse but also in terms of parallelization, because programmer's expertise is not demanded. Indeed, it is the compiler that entirely manages and optimizes algorithm parallelization. The experiments performed show that the transformation process does not negatively affect algorithm performance. © 2018 IEEE.},
	type = {Conference paper},
	source = {Scopus}
}

@ARTICLE{Di Martino20201920,
	author = {Di Martino, Beniamino and Esposito, Antonio and D'Angelo, Salvatore and Maisto, Salvatore Augusto and Nacchia, Stefania},
	title = {A compiler for agnostic programming and deployment of big data analytics on multiple platforms},
	year = {2020},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	volume = {30},
	number = {9},
	pages = {1920 – 1931},
	doi = {10.1109/TPDS.2019.2901488},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081117277&doi=10.1109%2fTPDS.2019.2901488&partnerID=40&md5=561771db57bcf4c4ce26324409809007},
	abstract = {To run proper Big Data Analytics, small and medium enterprises (SMEs) need to acquire expertise, hardware and software, which often translates to relevant initial investments for activities not directly connected to the company's business. To reduce such investments, the TOREADOR project proposes a Big Data Analytics framework which supports users in devising their own Big Data solutions by keeping the inherent costs at a minimum, and leveraging pre-existent knowledge and expertise. Among the objectives of the TOREADOR framework is supporting developers in parallelizing and deploying their Big Data algorithms, in order to develop their own analytics solutions. This paper describes the Code-Based approach, adopted within the TOREADOR framework to parallelize users' algorithms and deploy them on distributed platforms, via the annotation of parallelizable code portions with parallelization primitives. The approach, which relies on the guidance of Parallel Patterns to implement the parallelization, and on Skeletons to automatically build execution and deployment templates, is realized through a source-to-source Compiler, also described in the present paper. © 2019 IEEE.},
	type = {Article},
	source = {Scopus},
	note = {All Open Access, Bronze Open Access}
}

@ARTICLE{Di Martino2024,
	author = {Di Martino, Beniamino and Colucci Cante, Luigi and Graziano, Mariangela and D’Angelo, Salvatore and Esposito, Antonio and Lupi, Pietro and Ammendolia, Rosario},
	title = {A semantic-based methodology for the management of document workflows in e-government: a case study for judicial processes},
	year = {2024},
	journal = {Knowledge and Information Systems},
	doi = {10.1007/s10115-024-02077-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187871485&doi=10.1007%2fs10115-024-02077-8&partnerID=40&md5=8b7ba6942be39b26295a699ad1da967b},
	abstract = {Trial excessive duration is a common problem in Juridical systems worldwide, even if some countries seem to be more affected by it than others. The European Council has provided metrics and statistics to identify this problem and has pointed out solutions, such as the simplification of norms and the digitization of Juridical procedures. The Italian Telematic Civil Process (TCP) is an example of this digitization effort that has surely positively influenced the duration of Trials, their traceability and general complexity. However, there are still many possible actions that can be taken to simplify the work of Judges and Chancellors, and to support their daily operations in dealing with several Trials at once, and with the consistent number of documents that are involved in them. This paper presents a toolchain and a related methodology for the management of documentation attached to Trials, based on semantic technologies and Natural Language Processing techniques, which will help Judges in faster assessing the situation of each Trial they follow, and will also provide the means to identify potential correlations among different Juridical procedures. The methodology is tested against a case study, i.e. the compensation requests related to road accidents, which has been provided and described by Domain Experts from the Italian Ministry of Justice. © The Author(s) 2024.},
	type = {Article},
	source = {Scopus},
	note = {All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Di Martino2017400,
	author = {Di Martino, Beniamino and D'Angelo, Salvatore and Esposito, Antonio},
	title = {A platform for MBDAaaS based on patterns and skeletons: The python based algorithms compiler},
	year = {2017},
	journal = {Proceedings of the 2017 IEEE 14th International Conference on Networking, Sensing and Control, ICNSC 2017},
	pages = {400 – 405},
	doi = {10.1109/ICNSC.2017.8000126},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028515192&doi=10.1109%2fICNSC.2017.8000126&partnerID=40&md5=c2456a89d1659b34d601cea271b73a05},
	abstract = {Due to the initial investments needed by companies to acquire the necessary expertise, hardware and software to run proper Big Data Analytics, small and medium enterprises (SMEs) are often incapacitated from exploiting the full benefits of Big Data. The Model-Driven provided by Toreador promises to support SMEs in reaping the fruits of Big Data Analytics, by keeping the inherent costs at a minimum and thus allowing for a proper exploitation. In this paper, Models for declaring and describing algorithms, procedures and deployment configuration for Big Data are presented, with a focus on procedural aspects. © 2017 IEEE.},
	type = {Conference paper},
	source = {Scopus}
}

@ARTICLE{Di Martino2022,
	author = {Di Martino, Beniamino and Cante, Luigi Colucci and D’Angelo, Salvatore and Esposito, Antonio and Graziano, Mariangela and Marulli, Fiammetta and Lupi, Pietro and Cataldi, Alessandra},
	title = {A Big Data Pipeline and Machine Learning for Uniform Semantic Representation of Data and Documents from IT Systems of the Italian Ministry of Justice},
	year = {2022},
	journal = {International Journal of Grid and High Performance Computing},
	volume = {14},
	number = {1},
	doi = {10.4018/IJGHPC.301579},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149979731&doi=10.4018%2fIJGHPC.301579&partnerID=40&md5=6372a1c77a5141032bc01aa38e29d2f7},
	abstract = {In this paper, a big data pipeline is presented, taking in consideration both structured and unstructured data made available by the Italian Ministry of Justice, regarding their telematic civil process. Indeed, the complexity and volume of the data provided by the ministry requires the application of big data analysis techniques, in concert with machine and deep learning frameworks, to be correctly analysed and to obtain meaningful information that could support the ministry itself in better managing civil processes. The pipeline has two main objectives: to provide a consistent workflow of activities to be applied to the incoming data, aiming at extracting useful information for the ministry’s decision making tasks, and to homogenize the incoming data, so that they can be stored in a centralized and coherent data lake to be used as a reference for further analysis and considerations. Copyright © 2022, IGI Global.},
	type = {Article},
	source = {Scopus}
}

@CONFERENCE{Martino2016537,
	author = {Martino, Beniamino Di and Esposito, Antonio and D'Angelo, Salvatore and Marrazzo, Alessandro and Capasso, Angelo},
	title = {Automatic Production of an Ontology with NLP: Comparison between a Prolog Based Approach and a Cloud Approach Based on Bluemix Watson Service},
	year = {2016},
	journal = {Proceedings - 2016 10th International Conference on Complex, Intelligent, and Software Intensive Systems, CISIS 2016},
	pages = {537 – 542},
	doi = {10.1109/CISIS.2016.98},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011026278&doi=10.1109%2fCISIS.2016.98&partnerID=40&md5=56dd72aeb31c43401c34993e1aae81f1},
	abstract = {Nowadays, most of the information available on the web is in Natural Language. Extracting such knowledge from Natural Language text is an essential work and a very remarkable research topic in the Semantic Web field. The logic programming language Prolog, based on the definite-clause formalism, is a useful tool for implementing a Natural Language Processing (NLP) systems. However, web-based services for NLP have also been developed recently, and they represent an important alternative to be considered. In this paper we present the comparison between two different approaches in NLP, for the automatic creation of an OWL ontology supporting the semantic annotation of text. The first one is a pure Prolog approach, based on grammar and logic analysis rules. The second one is based on Watson Relationship Extraction service of IBM Cloud platform Bluemix. We evaluate the two approaches in terms of performance, the quality of NLP result, OWL completeness and richness. © 2016 IEEE.},
	type = {Conference paper},
	source = {Scopus}
}